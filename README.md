# WikiStream: Real-Time Wikipedia Edit Analytics Platform

A streaming data pipeline that captures live Wikipedia edits from Wikimedia EventStreams, processes them through a medallion architecture, and surfaces insights for content monitoring and risk detection.

**Tech Stack:** Amazon MSK (KRaft 3.9.x) Â· EMR Serverless (Spark 3.5) Â· AWS Step Functions Â· Amazon S3 Tables (Iceberg 1.10.0) Â· ECS Fargate Â· AWS Deequ  
**Deployment:** AWS Cloud (us-east-1) Â· Terraform 1.6+

> ğŸ“ **For detailed architecture diagrams and component breakdown, see [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)**

---

## Table of Contents

1. [Business Problem](#business-problem)
2. [Solution Overview](#solution-overview)
3. [Architecture](#architecture)
4. [Data Model](#data-model)
5. [Technology Choices](#technology-choices)
6. [Cost Estimation](#cost-estimation)
7. [Quick Start](#quick-start)
8. [Project Structure](#project-structure)

---

## Business Problem

### The Data Source

Wikimedia operates one of the largest collaborative platforms globally:

| Metric | Value |
|--------|-------|
| Wikipedia editions | 300+ languages |
| Edits per minute | ~500-700 (peak: 1,500+) |
| Monthly active editors | 280,000+ |
| Bot edits | ~20-30% of total |

**Wikimedia EventStreams** provides real-time Server-Sent Events (SSE) of all edits across all Wikimedia projects.

### The Use Cases

| Use Case | Description | Value |
|----------|-------------|-------|
| **Content Monitoring** | Track edit velocity, top contributors, trending pages | Understand community activity |
| **Vandalism Detection** | Identify suspicious edit patterns (rapid edits, large deletions) | Protect content quality |
| **Regional Analysis** | Compare activity across language editions | Geographic insights |
| **Bot Activity Tracking** | Monitor automated vs human contributions | Community health |

### Target Domains

We filter events from high-activity Wikipedia editions:

```
HIGH_ACTIVITY: en, de, ja, fr, zh, es, ru
REGIONAL: asia_pacific, europe, americas, middle_east
```

---

## Solution Overview

WikiStream implements the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) for progressive data refinement using **Apache Iceberg** format on **S3 Tables**:

```mermaid
flowchart LR
    subgraph Sources["ğŸ“¡ Source"]
        WM["Wikipedia<br/>EventStreams"]
    end

    subgraph Ingestion["ğŸ“¥ Ingestion"]
        ECS["ECS Fargate<br/>Producer"]
        MSK["Amazon MSK<br/>Kafka 3.9.x"]
    end

    subgraph Processing["âš¡ EMR Serverless"]
        BRONZE["ğŸ¥‰ Bronze<br/>Streaming"]
        SILVER["ğŸ¥ˆ Silver<br/>Batch"]
        DQ["ğŸ” Data Quality"]
        GOLD["ğŸ¥‡ Gold<br/>Batch"]
    end

    subgraph Storage["ğŸ’¾ S3 Tables"]
        ICE["Apache Iceberg<br/>Tables"]
    end

    WM -->|SSE| ECS
    ECS -->|Produce| MSK
    MSK -->|30s micro-batch| BRONZE
    BRONZE --> ICE
    ICE --> SILVER
    SILVER --> ICE
    ICE --> DQ
    ICE --> GOLD
    GOLD --> ICE
```

### Processing Flow

| Layer | Job Type | Trigger | Description |
|-------|----------|---------|-------------|
| **Bronze** | Streaming | 30s micro-batches | Kafka â†’ Iceberg with exactly-once semantics |
| **Silver** | Batch | Every 5 min (Step Functions) | Deduplication, normalization, enrichment |
| **Data Quality** | Batch | Every 5 min (Step Functions) | Deequ validation: completeness, validity |
| **Gold** | Batch | Every 5 min (Step Functions) | Aggregations: hourly stats, entity trends, risk scores |

### Key Features

- **â‰¤30 second** Bronze ingestion latency
- **â‰¤5 minute** end-to-end SLA for dashboard freshness
- **Exactly-once semantics** via Spark checkpointing and idempotent MERGE
- **Auto-recovery** Lambda restarts Bronze job on health check failure
- **Data quality gates** with Deequ before Gold layer updates

---

## Architecture

> ğŸ“ **See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for detailed architecture diagrams, component breakdown, and data flow visualizations.**

### High-Level Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              WikiStream Pipeline                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  INGESTION              STREAMING              PROCESSING           STORAGE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wikipedia  â”‚        â”‚  Amazon    â”‚        â”‚ EMR Serverless â”‚    â”‚ S3 Tablesâ”‚
â”‚ SSE Feed   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   MSK      â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚                â”‚â”€â”€â”€â–¶â”‚ (Iceberg)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  (Kafka)   â”‚        â”‚ Bronzeâ†’Silver  â”‚    â”‚          â”‚
      â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â†’DQâ†’Gold       â”‚    â”‚ bronze   â”‚
      â–¼                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ silver   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚              â”‚ gold     â”‚
â”‚ECS Fargate â”‚                                     â–¼              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Producer   â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚   Step     â”‚
                                            â”‚ Functions  â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Components

| Component | Service | Description |
|-----------|---------|-------------|
| **Data Source** | Wikipedia EventStreams | Real-time SSE feed |
| **Producer** | ECS Fargate | Python Kafka producer with IAM auth |
| **Message Broker** | Amazon MSK (KRaft 3.9.x) | 2 brokers, topics: `raw-events`, `dlq-events` |
| **Processing** | EMR Serverless (Spark 3.5) | Bronze streaming + Silver/Gold batch |
| **Storage** | S3 Tables (Iceberg 1.10.0) | Medallion architecture tables |
| **Orchestration** | Step Functions + EventBridge | Unified batch pipeline |
| **Auto-Recovery** | Lambda + CloudWatch | Bronze job health monitoring |
| **Monitoring** | CloudWatch + SNS | Dashboard, metrics, alerts |

---

## Data Model

### Wikimedia Event Schema

Sample event from `stream.wikimedia.org/v2/stream/recentchange`:

```json
{
  "meta": {
    "id": "c3b60285-58c0-493e-ad60-ddab7732fcc4",
    "domain": "en.wikipedia.org",
    "dt": "2025-01-01T10:00:00Z"
  },
  "type": "edit",
  "title": "Example_Article",
  "user": "Editor123",
  "bot": false,
  "length": {"old": 1000, "new": 1050},
  "revision": {"old": 123456, "new": 123457}
}
```

### Medallion Tables

```mermaid
erDiagram
    BRONZE_WIKI_EVENTS {
        string event_id PK
        bigint rc_id
        string event_type
        string domain
        string title
        string user
        boolean is_bot
        int length_delta
        timestamp event_timestamp
        string event_date "PARTITION"
        int event_hour "PARTITION"
    }

    SILVER_WIKI_EVENTS_CLEANED {
        string event_id PK
        string event_type
        string domain
        string region
        string language
        string user_normalized
        boolean is_bot
        boolean is_anonymous
        int length_delta
        boolean is_valid
        timestamp event_timestamp
        string event_date "PARTITION"
    }

    GOLD_HOURLY_EDIT_STATS {
        string stat_date PK "PARTITION"
        int stat_hour PK
        string domain PK
        bigint total_events
        bigint unique_users
        bigint bytes_added
        double bot_percentage
    }

    GOLD_RISK_SCORES {
        string stat_date PK "PARTITION"
        string entity_id PK
        double risk_score
        string risk_level
        string evidence
        boolean alert_triggered
    }

    BRONZE_WIKI_EVENTS ||--o{ SILVER_WIKI_EVENTS_CLEANED : "transforms to"
    SILVER_WIKI_EVENTS_CLEANED ||--o{ GOLD_HOURLY_EDIT_STATS : "aggregates to"
    SILVER_WIKI_EVENTS_CLEANED ||--o{ GOLD_RISK_SCORES : "generates"
```

### Risk Score Explained

The `gold.risk_scores` table identifies potentially problematic edit patterns:

| Risk Factor | Threshold | Points |
|-------------|-----------|--------|
| High edit velocity | >50 edits/hour | 40 |
| Large deletions | >3 large deletions | 30 |
| Anonymous activity | >50% anonymous | 20 |
| Cross-domain activity | >5 domains + high velocity | 10 |

**Risk Levels:**
- **HIGH** (70-100): Immediate attention needed
- **MEDIUM** (40-69): Monitor closely
- **LOW** (0-39): Normal activity

### Partitioning Strategy

| Layer | Partition Columns | Rationale |
|-------|-------------------|-----------|
| **Bronze** | `(event_date, event_hour)` | Streaming micro-batches need time-based partitioning |
| **Silver** | `(event_date)` | Date-based for efficient time-range queries |
| **Gold** | `(stat_date)` | Daily aggregations |

---

## Technology Choices

| Component | Service | Why This Choice |
|-----------|---------|-----------------|
| **Message Queue** | Amazon MSK (KRaft 3.9.x) | Kafka without Zookeeper complexity. Native AWS IAM auth. |
| **Stream Producer** | ECS Fargate | Long-running SSE consumer. Pay-per-second, no server management. |
| **Bronze Processing** | EMR Serverless (Streaming) | Spark Structured Streaming with exactly-once semantics. Auto-restart via Lambda. |
| **Silver/Gold Processing** | EMR Serverless (Batch) | Zero idle cost. Pay only during job execution. Auto-scaling Spark. |
| **Table Format** | S3 Tables (Iceberg 1.10.0) | AWS-managed Iceberg with ACID transactions, time-travel, automatic compaction. |
| **Data Quality** | AWS Deequ 2.0.7 | Native Spark integration. Completeness, validity, uniqueness checks. |
| **Orchestration** | Step Functions + EventBridge | Serverless. Unified batch pipeline (Silver â†’ DQ â†’ Gold) |
| **Auto-Recovery** | Lambda | Restarts Bronze job on CloudWatch alarm trigger |

### Why Not...?

| Alternative | Reason Not Used |
|-------------|-----------------|
| MSK Serverless | Less control over configuration; provisioned is more predictable |
| MWAA (Airflow) | $250+/month minimum; Step Functions is 90% cheaper |
| Lambda for Producer | 15-minute timeout; SSE stream requires continuous connection |
| Kinesis | MSK provides more flexibility with Kafka ecosystem |
| All Streaming | Silver/Gold transformations don't benefit from streaming; batch is more cost-effective |
| dbt | Adds operational complexity. Native Spark SQL + Step Functions provides equivalent functionality |

---

## Cost Estimation

### Monthly Cost (Dev/Portfolio)

| Service | Configuration | Est. Cost |
|---------|---------------|-----------|
| MSK | 2Ã— kafka.t3.small, 50GB each | ~$90 |
| ECS Fargate | 0.25 vCPU, 0.5GB, 24/7 (Producer) | ~$10 |
| EMR Serverless - Bronze | Streaming job, pre-warmed workers | ~$100 |
| EMR Serverless - Silver/Gold | Batch jobs via Step Functions | ~$15 |
| S3 Tables + S3 | ~10 GB storage | ~$3 |
| NAT Gateway | Single AZ | ~$35 |
| Step Functions + Lambda | Orchestration + auto-restart | ~$2 |
| CloudWatch | Logs + Metrics | ~$5 |
| **Total (24/7)** | | **~$260/month** |

### Cost Optimization Scripts

For development, use the provided scripts to destroy/recreate costly infrastructure:

```bash
# End of day - saves ~$19/day by destroying NAT, MSK, EMR
./scripts/destroy_infra.sh

# Start of day - recreates infrastructure (~25-35 min)
./scripts/create_infra.sh
```

**Preserved during destroy:** S3 buckets, S3 Tables (your data), ECR images, Terraform state

---

## Quick Start

### Prerequisites

```bash
aws --version          # AWS CLI v2.x required
terraform --version    # Terraform >= 1.6.0
docker --version       # Docker for building images
```

### Deploy

```bash
# 1. Setup Terraform backend (first time only)
./scripts/setup_terraform_backend.sh

# 2. Deploy infrastructure
cd infrastructure/terraform
terraform init
terraform apply

# 3. Build and push producer image
ECR_URL=$(terraform output -raw ecr_repository_url)
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $(echo $ECR_URL | cut -d'/' -f1)
cd ../../producer
docker build --platform linux/amd64 -t ${ECR_URL}:latest .
docker push ${ECR_URL}:latest

# 4. Upload Spark jobs
DATA_BUCKET=$(terraform -chdir=../infrastructure/terraform output -raw data_bucket)
aws s3 sync ../spark/jobs/ s3://${DATA_BUCKET}/spark/jobs/

# 5. Start services
./scripts/create_infra.sh  # Starts ECS producer + Bronze streaming job
```

### Verify Pipeline

```bash
# Check EMR jobs
EMR_APP_ID=$(terraform -chdir=infrastructure/terraform output -raw emr_serverless_app_id)
aws emr-serverless list-job-runs --application-id ${EMR_APP_ID} --output table

# Check producer logs
aws logs tail /ecs/wikistream-dev-producer --follow --since 5m

# Check Step Functions executions
aws stepfunctions list-executions \
  --state-machine-arn $(terraform -chdir=infrastructure/terraform output -raw batch_pipeline_state_machine_arn) \
  --max-results 5
```

---

## Project Structure

```
wikistream/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # Detailed architecture documentation
â”‚   â””â”€â”€ architecture_diagram.html      # Interactive HTML diagram
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf                    # AWS resources (VPC, MSK, EMR, ECS, Step Functions)
â”‚       â””â”€â”€ variables.tf               # Configuration variables
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ kafka_producer.py              # SSE consumer â†’ Kafka producer
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ Dockerfile                     # Container image for ECS
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_streaming_job.py    # Kafka â†’ Bronze (Spark Structured Streaming)
â”‚   â”‚   â”œâ”€â”€ silver_batch_job.py        # Bronze â†’ Silver (Spark Batch)
â”‚   â”‚   â”œâ”€â”€ gold_batch_job.py          # Silver â†’ Gold (Spark Batch)
â”‚   â”‚   â””â”€â”€ data_quality_job.py        # Deequ quality checks
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ bronze_schema.py           # Raw event schema
â”‚       â”œâ”€â”€ silver_schema.py           # Cleaned event schema
â”‚       â””â”€â”€ gold_schema.py             # Aggregation schemas
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                    # Domain filters, regions, SLAs
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_terraform_backend.sh     # Setup S3 + DynamoDB for state
â”‚   â”œâ”€â”€ create_infra.sh                # Start infrastructure + pipeline
â”‚   â”œâ”€â”€ destroy_infra.sh               # Stop costly resources (preserves data)
â”‚   â””â”€â”€ deploy.sh                      # Full deployment script
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ grafana/dashboards/            # Grafana dashboard exports
â”‚   â””â”€â”€ cloudwatch/alarms.json         # CloudWatch alarm definitions
â””â”€â”€ quicksight/
    â”œâ”€â”€ datasets/                      # QuickSight dataset configs
    â””â”€â”€ dashboards/                    # Dashboard definitions
```

---

## Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| **ECS Kafka Producer** | âœ… Implemented | Python, IAM auth, DLQ support |
| **MSK Cluster (KRaft)** | âœ… Implemented | Kafka 3.9.x, 2 brokers |
| **Bronze Streaming Job** | âœ… Implemented | 30s micro-batches, MERGE INTO |
| **Silver Batch Job** | âœ… Implemented | Deduplication, normalization |
| **Gold Batch Job** | âœ… Implemented | Aggregations, risk scores |
| **Data Quality (Deequ)** | âœ… Implemented | Completeness, validity checks |
| **Step Functions Pipeline** | âœ… Implemented | Unified: Silver â†’ DQ â†’ Gold |
| **Auto-Restart Lambda** | âœ… Implemented | CloudWatch alarm trigger |
| **CloudWatch Dashboard** | âœ… Implemented | Pipeline health metrics |
| **SNS Alerts** | âœ… Implemented | Failure notifications |
| **S3 Tables (Iceberg)** | âœ… Implemented | bronze, silver, gold namespaces |
| **Cost Optimization Scripts** | âœ… Implemented | destroy/create for dev workflow |

---

## Author

Built as a portfolio project demonstrating modern data engineering on AWS.

**Skills Demonstrated:**
- Real-time streaming with Kafka (MSK KRaft 3.9.x)
- Apache Iceberg 1.10.0 on S3 Tables
- Serverless Spark (EMR Serverless 7.12.0)
- Infrastructure as Code (Terraform)
- Medallion Architecture (Bronze streaming, Silver/Gold batch)
- Data Quality (AWS Deequ)
- Self-healing infrastructure (Lambda auto-restart)
- Cost-optimized dev workflow
