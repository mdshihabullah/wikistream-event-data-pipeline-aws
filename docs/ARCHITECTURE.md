# WikiStream Pipeline Architecture

> Real-time Wikipedia Edit Stream Processing on AWS using Medallion Architecture with Data Quality Gates

## ğŸ“Š Architecture Overview

```mermaid
flowchart TB
    subgraph EXT["â˜ï¸ EXTERNAL"]
        WIKI["ğŸŒ Wikipedia EventStreams<br/>Server-Sent Events"]
    end

    subgraph VPC["ğŸ”’ AWS VPC (us-east-1)"]
        subgraph INGEST["ğŸ“¥ INGESTION"]
            ECS["ğŸ³ ECS Fargate<br/>Kafka Producer<br/>0.25 vCPU / 512 MB"]
        end

        subgraph STREAM["ğŸ“¡ STREAMING"]
            MSK["Amazon MSK<br/>Kafka 3.9.x KRaft<br/>2x kafka.t3.small"]
        end

        subgraph PROCESS["âš¡ EMR SERVERLESS (Spark 3.5)"]
            BRONZE["ğŸ¥‰ Bronze Streaming<br/>3-min micro-batches"]
            BRONZE_DQ["ğŸ” Bronze DQ Gate"]
            SILVER["ğŸ¥ˆ Silver Batch"]
            SILVER_DQ["ğŸ” Silver DQ Gate"]
            GOLD["ğŸ¥‡ Gold Batch"]
            GOLD_DQ["ğŸ” Gold DQ Gate"]
        end
    end

    subgraph STORE["ğŸ’¾ STORAGE"]
        S3T["ğŸ“Š S3 Tables<br/>Apache Iceberg 1.10.0"]
        S3["ğŸ“¦ S3 Bucket<br/>Checkpoints / Logs"]
    end

    subgraph OPS["ğŸ“ˆ ORCHESTRATION & MONITORING"]
        SFN["âš™ï¸ Step Functions"]
        EB["â° EventBridge<br/>15-min initial"]
        CW["ğŸ“Š CloudWatch"]
        LAMBDA["Î» Auto-Restart"]
        SNS["ğŸ“§ SNS Alerts"]
    end

    WIKI -->|SSE| ECS
    ECS -->|Produce| MSK
    MSK -->|Consume| BRONZE
    BRONZE --> S3T
    BRONZE --> S3
    S3T --> BRONZE_DQ
    BRONZE_DQ -->|Pass| SILVER
    SILVER --> S3T
    S3T --> SILVER_DQ
    SILVER_DQ -->|Pass| GOLD
    GOLD --> S3T
    S3T --> GOLD_DQ
    EB --> SFN
    SFN --> BRONZE_DQ
    SFN --> SILVER
    SFN --> SILVER_DQ
    SFN --> GOLD
    SFN --> GOLD_DQ
    CW --> LAMBDA
    LAMBDA --> BRONZE
    BRONZE_DQ -->|Fail| SNS
    SILVER_DQ -->|Fail| SNS
    GOLD_DQ -->|Fail| SNS
```

## ğŸ—ï¸ Component Details

### Data Ingestion Layer

| Component | Technology | Configuration | Description |
|-----------|------------|---------------|-------------|
| **Data Source** | Wikipedia EventStreams | `stream.wikimedia.org` | Real-time SSE feed of Wikipedia edits |
| **Producer** | ECS Fargate (Python) | 0.25 vCPU, 512 MB | Consumes SSE, produces to Kafka with IAM auth |
| **Message Broker** | Amazon MSK (Kafka 3.9.x) | 2 brokers, KRaft mode | Topics: `raw-events`, `dlq-events` |

### Processing Layer (EMR Serverless)

| Job | Type | Trigger | Resource Allocation | Description |
|-----|------|---------|---------------------|-------------|
| **Bronze Streaming** | Spark Structured Streaming | 3-min micro-batches | 8 vCPU (2 driver + 2Ã—2 executor) | Kafka â†’ Iceberg with MERGE, 10-min watermark |
| **Bronze DQ Gate** | Batch | Step Functions | 4 vCPU (1 driver + 1Ã—2 executor) | Completeness, timeliness, validity checks |
| **Silver Batch** | Batch | Step Functions (5 min) | 4 vCPU | Deduplication, normalization, region mapping |
| **Silver DQ Gate** | Batch | Step Functions | 4 vCPU | Accuracy, consistency, drift detection |
| **Gold Batch** | Batch | Step Functions (5 min) | 4 vCPU | Hourly stats, entity trends, risk scores |
| **Gold DQ Gate** | Batch | Step Functions | 4 vCPU | Upstream verification, validation checks |

### Data Quality Gates (AWS Deequ / PyDeequ)

DQ checks are implemented using **AWS Deequ** (via PyDeequ 1.4.0 wrapper) for scalable data quality validation. Deequ provides unit tests for data with automatic constraint verification. Results are logged to `dq_audit.quality_results`:

| Layer | Check Type | Description | Blocking |
|-------|------------|-------------|----------|
| **Bronze** | Completeness | Critical fields (event_id, event_type, domain, event_timestamp) 100% | âœ… Yes |
| **Bronze** | Completeness | Important fields (title, user, wiki) â‰¥95% | âš ï¸ Warning |
| **Bronze** | Timeliness | 95th percentile event latency â‰¤60s | âœ… Yes |
| **Bronze** | Validity | event_type in allowed set, namespace â‰¥0, event_hour 0-23 | âœ… Yes |
| **Bronze** | Uniqueness | event_id unique within batch | âœ… Yes |
| **Silver** | Accuracy | length_delta = length_new - length_old (99%) | âœ… Yes |
| **Silver** | Accuracy | is_anonymous derived from IP pattern (99%) | âœ… Yes |
| **Silver** | Accuracy | Region mapping from domain (100%) | âœ… Yes |
| **Silver** | Consistency | is_valid flag = true for all Silver records | âœ… Yes |
| **Silver** | Drift | Null rate change >20% triggers alert | âš ï¸ Alert |
| **Gold** | Upstream | Bronze & Silver gates must pass | âœ… Yes |
| **Gold** | Consistency | total_events â‰¥ unique_users | âœ… Yes |
| **Gold** | Validity | bot_percentage 0-100, risk_score 0-100 | âœ… Yes |

### Storage Layer (S3 Tables with Apache Iceberg)

| Namespace | Tables | Partitioning | Description |
|-----------|--------|--------------|-------------|
| **bronze** | `raw_events` | (event_date, event_hour) | Raw ingested events from Kafka |
| **silver** | `cleaned_events` | (event_date, region) | Deduplicated, normalized, enriched |
| **gold** | `hourly_stats` | (stat_date, region) | Hourly aggregated metrics by domain |
| **gold** | `risk_scores` | (stat_date) | User-level risk scoring with evidence |
| **dq_audit** | `quality_results` | (run_date, layer) | DQ check results for audit trail |
| **dq_audit** | `profile_metrics` | (run_date, layer) | Column statistics for drift detection |

**Iceberg Table Properties:**
- Format version 3 with merge-on-read
- ZSTD compression
- 512 MB compaction target
- 48-hour snapshot retention (dev mode)

### Orchestration Layer

| Component | Technology | Configuration | Description |
|-----------|------------|---------------|-------------|
| **Batch Pipeline** | Step Functions | `wikistream-dev-batch-pipeline` | Bronze DQ â†’ Silver â†’ Silver DQ â†’ Gold â†’ Gold DQ |
| **Scheduler** | EventBridge | 15-min initial delay (serverless) | One-time trigger, then self-loops |
| **Auto-Recovery** | Lambda | Triggered by CloudWatch alarm | Restarts Bronze job on health check failure |
| **Alerts** | SNS | Email subscription | DQ gate failures, pipeline failures |
| **Dashboard** | CloudWatch | `wikistream-dev-pipeline-dashboard` | Pipeline metrics, DQ status, alarms |

## ğŸ“ Data Flow

```mermaid
---
config:
  layout: dagre
---
flowchart BT
    WIKI["ğŸŒ Wikipedia<br>EventStreams<br>SSE Stream<br>~500-700 edits/min"] -- SSE Stream --> PRODUCER["ğŸ³ ECS Fargate<br>Kafka Producer<br>Python 3.12<br>0.25 vCPU<br>DOMAIN FILTER<br>Allowed: 18 domains"]
    PRODUCER -- Produce event messages<br> --> MSK["ğŸ“¨ Amazon MSK<br>Kafka 3.9.x KRaft<br>2Ã— kafka.t3.small<br>Topics: raw-events, dlq-events"]
    PRODUCER -- Invalid events<br>Validation failures --> MSK_DLQ["dlq-events"]
    BRONZE["ğŸ¥‰ Bronze Streaming<br>EMR Serverless<br>Spark 3.5<br>3-min micro-batches<br>MERGE INTO<br>Exactly-once"] -- Consume --> MSK
    BRONZE -- Write --> S3T["ğŸ“Š S3 Tables<br>Apache Iceberg 1.10.0<br>Format v2, ZSTD<br>Bronze: raw_events"]
    BRONZE -- Metrics --> CW_METRICS["ğŸ“Š CloudWatch Metrics<br>BronzeRecordsProcessed<br>ProcessingLatencyMs<br>BatchCompleted"]
    CW_METRICS -- Health Check --> CW_ALARM["ğŸš¨ CloudWatch Alarm<br>No records in 10min"]
    CW_ALARM -- Trigger --> LAMBDA["Î» Lambda<br>Auto-Restart<br>Python 3.12"]
    LAMBDA -- Restart --> BRONZE
    BRONZE -- Fail --> SNS["ğŸ“§ SNS Topic<br>Email Alerts<br>Pipeline failures"]
    CW_METRICS -- Trigger --> EB["â° EventBridge<br>Schedule: 15min<br>Self-loops enabled"]
    EB -- Orchestrate --> SFN["âš™ï¸ Step Functions<br>Self-Looping Pipeline<br>~25-35 min cycle"]
    SFN -- Start --> BRONZE_DQ["ğŸ” Bronze DQ Gate<br>Deequ 2.0.7<br>Completeness, Timeliness<br>Validity, Uniqueness"] & SILVER["ğŸ¥ˆ Silver Batch<br>Deduplication<br>Region mapping<br>Anonymity detection<br>MERGE INTO"] & SILVER_DQ["ğŸ” Silver DQ Gate<br>Accuracy, Consistency<br>Drift detection"] & GOLD["ğŸ¥‡ Gold Batch<br>Hourly aggregations<br>Risk scoring 0-100<br>MERGE INTO"] & GOLD_DQ["ğŸ” Gold DQ Gate<br>Upstream validation<br>Aggregation consistency<br>Validity checks"]
    BRONZE_DQ -- Read --> S3T
    BRONZE_DQ -- Pass --> SFN
    BRONZE_DQ -- Fail --> SNS
    SILVER -- Read/Write --> S3T
    S3T -. Bronze table .-> SILVER
    SILVER -- Write --> S3T
    SILVER_DQ -- Read --> S3T
    SILVER_DQ -- Pass --> SFN
    SILVER_DQ -- Fail --> SNS
    GOLD -- Read/Write --> S3T
    S3T -. Silver table .-> GOLD
    GOLD -- Write --> S3T
    GOLD_DQ -- Read --> S3T
    GOLD_DQ -- Pass --> SFN
    GOLD_DQ -- Fail --> SNS
    S3T --> S3_BRONZE["bronze.raw_events<br>Partition: event_date, event_hour"] & S3_SILVER["silver.cleaned_events<br>Partition: event_date, region"] & S3_GOLD1["gold.hourly_stats<br>Partition: stat_date, region"] & S3_GOLD2["gold.risk_scores<br>Partition: stat_date"] & S3_GOLD3["gold.daily_analytics_summary<br>Partition: summary_date"] & S3_DQ1["dq_audit.quality_results<br>DQ gate evidence"] & S3_DQ2["dq_audit.profile_metrics<br>Drift detection data"]
    S3T -- Data Source --> DASHBOARD["ğŸ“Š CloudWatch Dashboard<br>Pipeline Health<br>DQ Status<br>SLA Monitoring"]
    S3T -. Business Analytics .-> QS["ğŸ“ˆ QuickSight Dashboard<br>Hourly Statistics<br>Risk Scores<br>Daily Analytics Summary"]
    SFN -- Wait 10min --> SFN

     WIKI:::external
     PRODUCER:::ingestion
     MSK:::Aqua
     MSK_DLQ:::Rose
     BRONZE:::Peach
     S3T:::storage
     CW_METRICS:::alert
     CW_ALARM:::alert
     LAMBDA:::Pine
     SNS:::alert
     EB:::orchestration
     SFN:::orchestration
     BRONZE_DQ:::Peach
     SILVER:::Ash
     SILVER_DQ:::Ash
     GOLD:::processing
     GOLD_DQ:::processing
     S3_BRONZE:::Peach
     S3_SILVER:::Ash
     S3_GOLD1:::external
     S3_GOLD2:::external
     S3_GOLD3:::external
     S3_DQ1:::storage
     S3_DQ2:::storage
     DASHBOARD:::dashboard
     QS:::dashboard
    classDef ingestion fill:#E1F5FE,stroke:#2196F3,stroke-width:2px,color:black
    classDef streaming fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px,color:black
    classDef monitoring fill:#FFEBEE,stroke:#E53935,stroke-width:2px,color:black
    classDef storage fill:#E1F5FE,stroke:#2196F3,stroke-width:2px,color:black
    classDef orchestration fill:#F3E5F5,stroke:#7B1FA2,stroke-width:2px,color:black
    classDef alert fill:#FFEBEE,stroke:#C62828,stroke-width:2px,color:black
    classDef analytics fill:#FAFAFA,stroke:#BDBDBD,stroke-width:2px,color:#9E9E9E,stroke-dasharray: 5 5
    classDef dashboard fill:#E8F5E9, stroke:#2E7D32, stroke-width:2px, color:black
    classDef processing fill:#FFF9C4, stroke:#F9A825, stroke-width:2px, color:black
    classDef Peach stroke-width:1px, stroke-dasharray:none, stroke:#FBB35A, fill:#FFEFDB, color:#8F632D
    classDef Ash stroke-width:1px, stroke-dasharray:none, stroke:#999999, fill:#EEEEEE, color:#000000
    classDef external fill:#FFE5B4, stroke:#E85A23, stroke-width:3px, color:black
    classDef Sky stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    classDef Pine stroke-width:1px, stroke-dasharray:none, stroke:#254336, fill:#27654A, color:#FFFFFF
    classDef Rose stroke-width:1px, stroke-dasharray:none, stroke:#FF5978, fill:#FFDFE5, color:#8E2236
```

### Data Flow Key Points

1. **Domain Filtering**: Occurs at Producer (ECS Fargate) before Kafka - only 18 allowed domains enter pipeline
2. **Bronze Streaming**: Continuous 3-min micro-batches from Kafka, writes to Iceberg with MERGE for idempotency
3. **Batch Pipeline**: Self-looping Step Functions orchestrates Silver â†’ Silver DQ â†’ Gold â†’ Gold DQ every ~25-35 min
4. **DQ Gates**: Block downstream on failure, alert via SNS, all evidence logged to `dq_audit` tables
5. **Auto-Recovery**: Lambda restarts Bronze job on health check failure
6. **Monitoring**: CloudWatch metrics, alarms, and comprehensive dashboard for pipeline health
7. **Storage**: All data in S3 Tables (Iceberg v2) with ZSTD compression and auto-compaction
8. **Analytics**: QuickSight is provisioned via Terraform with datasets for hourly_stats, risk_scores, daily_analytics_summary, and silver.cleaned_events

## ğŸ”§ Technology Stack

| Category | Technologies |
|----------|--------------|
| **Compute** | EMR Serverless (Spark 3.5, emr-7.12.0), ECS Fargate, Lambda |
| **Streaming** | Amazon MSK (Kafka 3.9.x, KRaft mode, IAM auth) |
| **Table Format** | Apache Iceberg 1.10.0 via S3 Tables |
| **Data Quality** | AWS Deequ 2.0.7 + PyDeequ 1.4.0 with audit logging |
| **Languages** | Python 3.12, PySpark, SQL |
| **Infrastructure** | Terraform 1.6+, AWS Provider 5.80+ |
| **Orchestration** | AWS Step Functions, EventBridge |
| **Monitoring** | CloudWatch (Dashboard + Alarms), SNS, Grafana (local) |

## âš¡ Key Implementation Details

### Bronze Layer (Streaming)
- **Trigger Interval**: 3 minutes (reduced from 30s to minimize Iceberg snapshots)
- **Watermark Delay**: 10 minutes for late event handling
- **Deduplication**: Deterministic `event_id` with `MERGE INTO` for idempotent upserts
- **Schema Version**: Tracked for evolution support

### Silver Layer (Batch)
- **Region Mapping**: Domain â†’ region (asia_pacific, europe, americas, middle_east, other)
- **Anonymity Detection**: IP address pattern matching
- **Quality Flags**: `is_valid`, `is_large_deletion`, `is_large_addition`
- **Processing**: Only valid events pass to Silver

### Gold Layer (Batch)
- **Hourly Stats**: Volume, content, user, and edit type metrics by domain/region
- **Risk Scores**: User-level scoring (0-100) based on edit velocity, large deletions, cross-domain activity
- **Risk Levels**: LOW/MEDIUM/HIGH with evidence JSON for alerting

### DQ Gate Pipeline Flow
```
EventBridge (5 min) â†’ Step Functions:
  1. Bronze DQ Gate (validates recent Bronze data)
     â†“ Pass
  2. Silver Batch Job (transforms Bronze â†’ Silver)
     â†“ 
  3. Silver DQ Gate (validates Silver, checks drift)
     â†“ Pass
  4. Gold Batch Job (aggregates Silver â†’ Gold)
     â†“
  5. Gold DQ Gate (validates upstream + Gold)
     â†“ Pass
  âœ… Success
  
  Any failure â†’ SNS Alert â†’ Pipeline Fails
```

## ğŸ¯ SLA Targets

| Metric | Target | Implementation |
|--------|--------|----------------|
| Bronze Ingestion | â‰¤3 minutes | Spark Streaming trigger interval |
| Event Freshness | 95% <1 minute | Timeliness check in Bronze DQ |
| End-to-End | â‰¤5 minutes | Sequential Step Functions pipeline |
| DQ Gate Execution | Every 5 minutes | EventBridge schedule |
| Auto-Recovery | <10 minutes | Lambda restarts Bronze on failure |

## ğŸš€ Deployment

### Quick Start
```bash
# Create all infrastructure (~25-35 minutes for MSK)
./scripts/create_infra.sh

# Enable batch pipeline with DQ gates
aws events enable-rule --name wikistream-dev-batch-pipeline-schedule

# Start local Grafana monitoring
cd monitoring/docker && docker-compose up -d
# Open http://localhost:3000 (admin/wikistream)
```

### Teardown
```bash
# Partial destroy (preserves data)
./scripts/destroy_infra.sh

# Full destroy (removes everything)
./scripts/destroy_all.sh
```

## ğŸ“ Project Structure

```
wikistream/
â”œâ”€â”€ producer/                    # ECS Fargate Kafka producer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_streaming_job.py   # Kafka â†’ Bronze Iceberg
â”‚   â”‚   â”œâ”€â”€ silver_batch_job.py       # Bronze â†’ Silver
â”‚   â”‚   â”œâ”€â”€ gold_batch_job.py         # Silver â†’ Gold
â”‚   â”‚   â”œâ”€â”€ bronze_dq_gate.py         # Bronze DQ checks
â”‚   â”‚   â”œâ”€â”€ silver_dq_gate.py         # Silver DQ checks
â”‚   â”‚   â”œâ”€â”€ gold_dq_gate.py           # Gold DQ checks
â”‚   â”‚   â””â”€â”€ dq/                       # DQ module (packaged as dq.zip)
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ dq_checks.py          # Check implementations
â”‚   â”‚       â””â”€â”€ dq_utils.py           # Audit, metrics, alerts
â”‚   â””â”€â”€ schemas/
â”œâ”€â”€ infrastructure/terraform/    # IaC (VPC, MSK, EMR, S3 Tables, Step Functions)
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ docker/                  # Local Grafana setup
â”‚   â””â”€â”€ grafana/dashboards/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_infra.sh          # Full deployment
â”‚   â”œâ”€â”€ destroy_infra.sh         # Partial teardown
â”‚   â””â”€â”€ destroy_all.sh           # Full teardown
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md          # This file
    â””â”€â”€ architecture_diagram.html
```

---

*Architecture Document v2.1 - Accurate representation of implemented system*
